{
 "metadata": {
  "name": "",
  "signature": "sha256:e460e01729af2419d88a4e705d96c5308efc2c99980e046383974cd64a8d64b7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.path.insert(0, \"./dataProcess\")\n",
      "from stream import preprocess\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "# from reasoning import Reason\n",
      "import logging\n",
      "\n",
      "import config\n",
      "import numpy\n",
      "from utils import *\n",
      "config = getattr(config, 'get_config')()\n",
      "train_files = config['train_file']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from theano.tensor.nnet import categorical_crossentropy\n",
      "class LogisticRegression(object):\n",
      "    def __init__(self, input, n_in, n_out):\n",
      "\n",
      "        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
      "        self.W = param_init().uniform((n_in, n_out))\n",
      "        # initialize the baises b as a vector of n_out 0s\n",
      "        self.b = param_init().constant((n_out,))\n",
      "\n",
      "        # compute vector of class-membership probabilities in symbolic form\n",
      "        energy = theano.dot(input, self.W) + self.b\n",
      "        if energy.ndim == 3:\n",
      "            energy_exp = T.exp(energy - T.max(energy, 2, keepdims=True))\n",
      "            pmf = energy_exp / energy_exp.sum(2, keepdims=True)\n",
      "        else:\n",
      "            pmf = T.nnet.softmax(energy)\n",
      "\n",
      "        self.p_y_given_x = pmf\n",
      "        self.y_pred = T.argmax(self.p_y_given_x, axis=-1)\n",
      "\n",
      "        # compute prediction as class whose probability is maximal in\n",
      "        # symbolic form\n",
      "\n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "    def cost(self, targets, mask=None):\n",
      "        prediction = self.p_y_given_x # (9,5,24)\n",
      "\n",
      "        if prediction.ndim == 3:\n",
      "            # prediction = prediction.dimshuffle(1,2,0).flatten(2).dimshuffle(1,0)\n",
      "            prediction_flat = prediction.reshape(((prediction.shape[0] *\n",
      "                                                   prediction.shape[1]),\n",
      "                                                  prediction.shape[2]), ndim=2) #(45,24)\n",
      "            targets_flat = targets.flatten()\n",
      "            mask_flat = mask.flatten()\n",
      "            ce = categorical_crossentropy(prediction_flat, targets_flat) * mask_flat\n",
      "        else:\n",
      "            ce = categorical_crossentropy(prediction, targets)\n",
      "        return T.sum(ce)\n",
      "\n",
      "     #liuxianggen\n",
      "    def cost_entry(self, targets, mask=None):\n",
      "        prediction = self.p_y_given_x # (9,5,24)\n",
      "\n",
      "        if prediction.ndim == 3:\n",
      "            # prediction = prediction.dimshuffle(1,2,0).flatten(2).dimshuffle(1,0)\n",
      "            prediction_flat = prediction.reshape(((prediction.shape[0] *\n",
      "                                                   prediction.shape[1]),\n",
      "                                                  prediction.shape[2]), ndim=2) #(45,24)\n",
      "            targets_flat = targets.flatten()\n",
      "            mask_flat = mask.flatten()\n",
      "            ce = categorical_crossentropy(prediction_flat, targets_flat) * mask_flat\n",
      "        else:\n",
      "            ce = categorical_crossentropy(prediction, targets)\n",
      "        ce_entry = ce.reshape((prediction.shape[0],prediction.shape[1]), ndim=2).sum(axis=0)  #(5)\n",
      "        return ce_entry\n",
      "\n",
      "\n",
      "    def errors(self, y):\n",
      "        y_pred = self.y_pred\n",
      "        if y.ndim == 2:\n",
      "            y = y.flatten()\n",
      "            y_pred = y_pred.flatten()\n",
      "        return T.sum(T.neq(y, y_pred))\n",
      "\n",
      "\n",
      "class GRU(object):\n",
      "    def __init__(self, n_in, n_hids, with_contex=False, **kwargs):\n",
      "        self.n_in = n_in\n",
      "        self.n_hids = n_hids\n",
      "        self.with_contex = with_contex\n",
      "        if self.with_contex:\n",
      "            self.c_hids = kwargs.pop('c_hids', n_hids)\n",
      "        self._init_params()\n",
      "\n",
      "    def _init_params(self):\n",
      "        n_in = self.n_in\n",
      "        n_hids = self.n_hids\n",
      "        size_xh = (n_in, n_hids)\n",
      "        size_hh = (n_hids, n_hids)\n",
      "        self.W_xz = param_init().uniform(size_xh)\n",
      "        self.W_xr = param_init().uniform(size_xh)\n",
      "        self.W_xh = param_init().uniform(size_xh)\n",
      "\n",
      "        self.W_hz = param_init().orth(size_hh)\n",
      "        self.W_hr = param_init().orth(size_hh)\n",
      "        self.W_hh = param_init().orth(size_hh)\n",
      "\n",
      "        self.b_z = param_init().constant((n_hids,))\n",
      "        self.b_r = param_init().constant((n_hids,))\n",
      "        self.b_h = param_init().constant((n_hids,))\n",
      "\n",
      "        self.params = [self.W_xz, self.W_xr, self.W_xh,\n",
      "                       self.W_hz, self.W_hr, self.W_hh,\n",
      "                       self.b_z, self.b_r, self.b_h]\n",
      "\n",
      "        if self.with_contex:\n",
      "            size_ch = (self.c_hids, self.n_hids)\n",
      "            self.W_cz = param_init().uniform(size_ch)\n",
      "            self.W_cr = param_init().uniform(size_ch)\n",
      "            self.W_ch = param_init().uniform(size_ch)\n",
      "            self.W_c_init = param_init().uniform(size_ch)\n",
      "\n",
      "            self.params = self.params + [self.W_cz, self.W_cr,\n",
      "                                         self.W_ch, self.W_c_init]\n",
      "\n",
      "    def _step_forward_with_context(self, x_t, x_m, h_tm1, c_z, c_r, c_h):\n",
      "        '''\n",
      "        x_t: input at time t\n",
      "        x_m: mask of x_t\n",
      "        h_tm1: previous state\n",
      "        c_x: contex of the rnn\n",
      "        '''\n",
      "        z_t = T.nnet.sigmoid(T.dot(x_t, self.W_xz) +\n",
      "                             T.dot(h_tm1, self.W_hz) + c_z + self.b_z)\n",
      "\n",
      "        r_t = T.nnet.sigmoid(T.dot(x_t, self.W_xr) +\n",
      "                             T.dot(h_tm1, self.W_hr) + c_r + self.b_r)\n",
      "\n",
      "        can_h_t = T.tanh(T.dot(x_t, self.W_xh) +\n",
      "                         r_t * T.dot(h_tm1, self.W_hh) + c_h +\n",
      "                         self.b_h)\n",
      "        h_t = (1 - z_t) * h_tm1 + z_t * can_h_t\n",
      "\n",
      "        h_t = x_m[:, None] * h_t + (1. - x_m[:, None]) * h_tm1\n",
      "        return h_t\n",
      "\n",
      "    def _step_forward(self, x_t, x_m, h_tm1):\n",
      "        '''\n",
      "        x_t: input at time t\n",
      "        x_m: mask of x_t\n",
      "        h_tm1: previous state\n",
      "        c_x: contex of the rnn\n",
      "        '''\n",
      "        z_t = T.nnet.sigmoid(T.dot(x_t, self.W_xz) +\n",
      "                             T.dot(h_tm1, self.W_hz) + self.b_z)\n",
      "\n",
      "        r_t = T.nnet.sigmoid(T.dot(x_t, self.W_xr) +\n",
      "                             T.dot(h_tm1, self.W_hr) + self.b_r)\n",
      "\n",
      "        can_h_t = T.tanh(T.dot(x_t, self.W_xh) +\n",
      "                         r_t * T.dot(h_tm1, self.W_hh) +\n",
      "                         self.b_h)\n",
      "        h_t = (1 - z_t) * h_tm1 + z_t * can_h_t\n",
      "\n",
      "        h_t = x_m[:, None] * h_t + (1. - x_m[:, None]) * h_tm1\n",
      "        return h_t\n",
      "\n",
      "    def apply(self, state_below, mask_below, init_state=None, context=None):\n",
      "        if state_below.ndim == 3:\n",
      "            batch_size = state_below.shape[1]\n",
      "            n_steps = state_below.shape[0]\n",
      "        else:\n",
      "            raise NotImplementedError\n",
      "\n",
      "        if self.with_contex:\n",
      "            if init_state is None:\n",
      "                init_state = T.tanh(theano.dot(context, self.W_c_init))\n",
      "            c_z = theano.dot(context, self.W_cz)\n",
      "            c_r = theano.dot(context, self.W_cr)\n",
      "            c_h = theano.dot(context, self.W_ch)\n",
      "            non_sequences = [c_z, c_r, c_h]\n",
      "            rval, updates = theano.scan(self._step_forward_with_context,\n",
      "                                        sequences=[state_below, mask_below],\n",
      "                                        outputs_info=[init_state],\n",
      "                                        non_sequences=non_sequences,\n",
      "                                        n_steps=n_steps\n",
      "                                        )\n",
      "\n",
      "        else:\n",
      "            if init_state is None:\n",
      "                init_state = T.alloc(numpy.float32(0.), batch_size, self.n_hids)\n",
      "            rval, updates = theano.scan(self._step_forward,\n",
      "                                        sequences=[state_below, mask_below],\n",
      "                                        outputs_info=[init_state],\n",
      "                                        n_steps=n_steps\n",
      "                                        )\n",
      "        self.output = rval\n",
      "        return self.output\n",
      "\n",
      "    def merge_out(self, state_below, mask_below, context=None):\n",
      "        hiddens = self.apply(state_below, mask_below, context=context)\n",
      "        if context is None:\n",
      "            msize = self.n_in + self.n_hids\n",
      "            osize = self.n_hids\n",
      "            combine = T.concatenate([state_below, hiddens], axis=2)\n",
      "        else:\n",
      "            msize = self.n_in + self.n_hids + self.c_hids\n",
      "            osize = self.n_hids\n",
      "            n_times = state_below.shape[0]\n",
      "            m_context = repeat_x(context, n_times)\n",
      "            combine = T.concatenate([state_below, hiddens, m_context], axis=2)\n",
      "\n",
      "        self.W_m = param_init().uniform((msize, osize * 2))\n",
      "        self.b_m = param_init().constant((osize * 2,))\n",
      "        self.params += [self.W_m, self.b_m]\n",
      "\n",
      "        merge_out = theano.dot(combine, self.W_m) + self.b_m\n",
      "        merge_max = merge_out.reshape((merge_out.shape[0],\n",
      "                                       merge_out.shape[1],\n",
      "                                       merge_out.shape[2] / 2,\n",
      "                                       2), ndim=4).max(axis=3)\n",
      "        return merge_max * mask_below[:, :, None]\n",
      "\n",
      "\n",
      "class lookup_table(object):\n",
      "    def __init__(self, embsize, vocab_size):\n",
      "        self.W = param_init().uniform((vocab_size, embsize))\n",
      "        self.params = [self.W]\n",
      "        self.vocab_size = vocab_size\n",
      "        self.embsize = embsize\n",
      "\n",
      "    def apply(self, indices):\n",
      "        outshape = [indices.shape[i] for i\n",
      "                    in range(indices.ndim)] + [self.embsize]\n",
      "\n",
      "        return self.W[indices.flatten()].reshape(outshape)\n",
      "\n",
      "\n",
      "class auto_encoder(object):\n",
      "    def __init__(self, sentence, sentence_mask, vocab_size, n_in, n_hids, **kwargs):\n",
      "        '''\n",
      "\n",
      "        :param sentence: \uff0810,5\uff09\n",
      "        :param sentence_mask:\n",
      "        :param vocab_size:\n",
      "        :param n_in:\n",
      "        :param n_hids:\n",
      "        :param kwargs:\n",
      "        :return:\n",
      "        '''\n",
      "        layers = []\n",
      "\n",
      "        # batch_size = sentence.shape[1]\n",
      "        encoder = GRU(n_in, n_hids, with_contex=False)\n",
      "        layers.append(encoder)\n",
      "\n",
      "        if 'table' in kwargs:\n",
      "            table = kwargs['table']\n",
      "        else:\n",
      "            table = lookup_table(n_in, vocab_size)\n",
      "        # layers.append(table)\n",
      "\n",
      "        state_below = table.apply(sentence)\n",
      "        momeries = encoder.apply(state_below, sentence_mask)  # (10,5,39)\n",
      "        context = momeries[-1]\n",
      "\n",
      "        decoder = GRU(n_in, n_hids, with_contex=True)\n",
      "        layers.append(decoder)\n",
      "\n",
      "        decoder_state_below = table.apply(sentence[:-1])\n",
      "        hiddens = decoder.merge_out(decoder_state_below,\n",
      "                                    sentence_mask[:-1], context=context) #(9,5,39)\n",
      "\n",
      "        logistic_layer = LogisticRegression(hiddens, n_hids, vocab_size)\n",
      "        layers.append(logistic_layer)\n",
      "        self.cost_entry = logistic_layer.cost_entry(sentence[1:], #(9,5)\n",
      "                                        sentence_mask[1:]) # predict model cost, (5)\n",
      "        self.cost = logistic_layer.cost(sentence[1:],\n",
      "                                        sentence_mask[1:]) / sentence_mask[1:].sum()  # predict model cost\n",
      "        self.output = momeries\n",
      "        self.params = []\n",
      "        for layer in layers:\n",
      "            self.params.extend(layer.params)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Memory(object):\n",
      "    \"\"\"\n",
      "    author:liuxianggen\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, **kwargs):\n",
      "        self.vocab_size = kwargs.pop('vocab_size')  #24\n",
      "        self.n_in = kwargs.pop('nemb') #30\n",
      "        self.n_hids = kwargs.pop('nhids') #39\n",
      "        self.n_layer = kwargs.pop('n_layer') # 3\n",
      "        self.n_label = kwargs.pop('label_size') # 12\n",
      "        self.params = []\n",
      "        self.cost = 0\n",
      "\n",
      "    def apply(self, facts, facts_mask, question, question_mask, y):\n",
      "        table = lookup_table(self.n_in, self.vocab_size)\n",
      "        self.params += table.params\n",
      "        facts_encoder = auto_encoder(facts, facts_mask, self.vocab_size,\n",
      "                                     self.n_in, self.n_hids, table=table)\n",
      "        self.params += facts_encoder.params\n",
      "        self.eval = facts_encoder.output  # (10,5,39)\n",
      "        self.cost = facts_encoder.cost #a float\n",
      "        self.cost_entry = facts_encoder.cost_entry # vector of float (5)\n",
      "        return self.eval  # (10,5,39)\n",
      "\n",
      "    # read the specific line of  fact and its loss in the memory \n",
      "    def read(self, index):\n",
      "\t\treturn self.eval[:,index,:],self.cost_entry[index]\n",
      "\n",
      "\n",
      "class Question(object):\n",
      "    def __init__(self, **kwargs):\n",
      "        self.vocab_size = kwargs.pop('vocab_size')  #24\n",
      "        self.n_in = kwargs.pop('nemb') #30\n",
      "        self.n_hids = kwargs.pop('nhids') #39\n",
      "        self.n_layer = kwargs.pop('n_layer') # 3\n",
      "        self.n_label = kwargs.pop('label_size') # 12\n",
      "        self.params = []\n",
      "\n",
      "    def apply(self, facts, facts_mask, question, question_mask, y):\n",
      "        table = lookup_table(self.n_in, self.vocab_size)\n",
      "        self.params += table.params\n",
      "        facts_encoder = auto_encoder(facts, facts_mask, self.vocab_size,\n",
      "                                     self.n_in, self.n_hids, table=table)\n",
      "        self.params += facts_encoder.params\n",
      "        self.eval = facts_encoder.output  # (13,39)\n",
      "        self.cost = facts_encoder.cost\n",
      "        return self.eval"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from utils import adadelta\n",
      "x = T.lmatrix()\n",
      "x_mask = T.matrix()\n",
      "y = T.lmatrix()\n",
      "y_mask = T.matrix()\n",
      "l = T.lvector()\n",
      "\n",
      "\n",
      "memory = Memory(**config)\n",
      "context = memory.apply(x, x_mask, y, y_mask, l) #(10,5,39)\n",
      "gcost = memory.cost\n",
      "# add regularization\n",
      "params = memory.params\n",
      "\n",
      "for param in params:\n",
      "    gcost += T.sum(param ** 2) * 1e-6  #l2\n",
      "    gcost += T.sum(abs(param)) * 1e-6  #l1\n",
      "grads = T.grad(gcost, params)\n",
      "updates = adadelta(params, grads)\n",
      "fn = theano.function([x, x_mask, y, y_mask, l], [memory.cost_entry], updates=updates,on_unused_input='ignore')\n",
      "\n",
      "data_class = preprocess(*train_files)\n",
      "# test_class = preprocess(*test_files)\n",
      "for _ in range(1):\n",
      "#     sums = 0.0\n",
      "    i = 0\n",
      "    for facts, question, label in data_class.data_stream():\n",
      "        sums = fn(facts[0].T, facts[1].T, question[0].T, question[1].T, label)[0]\n",
      "        print sums\n",
      "        break\n",
      "#         i += label.shape[0]\n",
      "#         print \"train error: {}\".format(float(sums/float(i)))\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 28.60250854  28.60250473  28.60250664  28.60251427  28.60250473]\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"train error: {}\".format(3231)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "train error: 3231\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random.randint(9)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 65,
       "text": [
        "2"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 9 21  0  2 14 20 21 16 11  5]\n",
        " [ 9 21 19  2 17 20 21  0 11  5]\n",
        " [ 9 21 18  2 10 20 21  3 11  5]\n",
        " [ 9 21  0  2 10 20 21 18 11  5]\n",
        " [ 9 21  6  2 17 20 21 18 11  5]]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}