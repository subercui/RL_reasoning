{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.path.insert(0, \"./dataProcess\")\n",
      "from stream import preprocess\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "# from reasoning import Reason\n",
      "import logging\n",
      "# from utils import adadelta\n",
      "import config\n",
      "import numpy\n",
      "from utils import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "class LogisticRegression(object):\n",
      "\n",
      "    def __init__(self, input, n_in, n_out):\n",
      "\n",
      "        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
      "        self.W = param_init().uniform((n_in, n_out))\n",
      "        # initialize the baises b as a vector of n_out 0s\n",
      "        self.b = param_init().constant((n_out, ))\n",
      "\n",
      "        # compute vector of class-membership probabilities in symbolic form\n",
      "        energy = theano.dot(input, self.W) + self.b\n",
      "        if energy.ndim == 3:\n",
      "            energy_exp = T.exp(energy - T.max(energy, 2, keepdims=True))\n",
      "            pmf = energy_exp / energy_exp.sum(2, keepdims=True)\n",
      "        else:\n",
      "            pmf = T.nnet.softmax(energy)\n",
      "\n",
      "        self.p_y_given_x = pmf\n",
      "        self.y_pred = T.argmax(self.p_y_given_x, axis=-1)\n",
      "\n",
      "        # compute prediction as class whose probability is maximal in\n",
      "        # symbolic form\n",
      "\n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "    def cost(self, targets, mask=None):\n",
      "        prediction = self.p_y_given_x\n",
      "\n",
      "        if prediction.ndim == 3:\n",
      "        # prediction = prediction.dimshuffle(1,2,0).flatten(2).dimshuffle(1,0)\n",
      "            prediction_flat = prediction.reshape(((prediction.shape[0] *\n",
      "                                                prediction.shape[1]),\n",
      "                                                prediction.shape[2]), ndim=2)\n",
      "            targets_flat = targets.flatten()\n",
      "            mask_flat = mask.flatten()\n",
      "            ce = categorical_crossentropy(prediction_flat, targets_flat) * mask_flat\n",
      "        else:\n",
      "            ce = categorical_crossentropy(prediction, targets)\n",
      "        return T.sum(ce)\n",
      "\n",
      "    def errors(self, y):\n",
      "        y_pred = self.y_pred\n",
      "        if y.ndim == 2:\n",
      "            y = y.flatten()\n",
      "            y_pred = y_pred.flatten()\n",
      "        return T.sum(T.neq(y, y_pred))\n",
      "\n",
      "\n",
      "\n",
      "class GRU(object):\n",
      "\n",
      "    def __init__(self, n_in, n_hids, with_contex=False, **kwargs):\n",
      "        self.n_in = n_in\n",
      "        self.n_hids = n_hids\n",
      "        self.with_contex = with_contex\n",
      "        if self.with_contex:\n",
      "            self.c_hids = kwargs.pop('c_hids', n_hids)\n",
      "        self._init_params()\n",
      "\n",
      "    def _init_params(self):\n",
      "        n_in = self.n_in\n",
      "        n_hids = self.n_hids\n",
      "        size_xh = (n_in, n_hids)\n",
      "        size_hh = (n_hids, n_hids)\n",
      "        self.W_xz = param_init().uniform(size_xh)\n",
      "        self.W_xr = param_init().uniform(size_xh)\n",
      "        self.W_xh = param_init().uniform(size_xh)\n",
      "\n",
      "        self.W_hz = param_init().orth(size_hh)\n",
      "        self.W_hr = param_init().orth(size_hh)\n",
      "        self.W_hh = param_init().orth(size_hh)\n",
      "\n",
      "        self.b_z = param_init().constant((n_hids,))\n",
      "        self.b_r = param_init().constant((n_hids,))\n",
      "        self.b_h = param_init().constant((n_hids,))\n",
      "\n",
      "        self.params = [self.W_xz, self.W_xr, self.W_xh,\n",
      "                       self.W_hz, self.W_hr, self.W_hh,\n",
      "                       self.b_z, self.b_r, self.b_h]\n",
      "\n",
      "        if self.with_contex:\n",
      "            size_ch = (self.c_hids, self.n_hids)\n",
      "            self.W_cz = param_init().uniform(size_ch)\n",
      "            self.W_cr = param_init().uniform(size_ch)\n",
      "            self.W_ch = param_init().uniform(size_ch)\n",
      "            self.W_c_init = param_init().uniform(size_ch)\n",
      "\n",
      "            self.params = self.params + [self.W_cz, self.W_cr,\n",
      "                                         self.W_ch, self.W_c_init]\n",
      "\n",
      "    def _step_forward_with_context(self, x_t, x_m, h_tm1, c_z, c_r, c_h):\n",
      "        '''\n",
      "        x_t: input at time t\n",
      "        x_m: mask of x_t\n",
      "        h_tm1: previous state\n",
      "        c_x: contex of the rnn\n",
      "        '''\n",
      "        z_t = T.nnet.sigmoid(T.dot(x_t, self.W_xz) +\n",
      "                             T.dot(h_tm1, self.W_hz) + c_z + self.b_z)\n",
      "\n",
      "        r_t = T.nnet.sigmoid(T.dot(x_t, self.W_xr) +\n",
      "                             T.dot(h_tm1, self.W_hr) + c_r + self.b_r)\n",
      "\n",
      "        can_h_t = T.tanh(T.dot(x_t, self.W_xh) +\n",
      "                         r_t * T.dot(h_tm1, self.W_hh) + c_h +\n",
      "                         self.b_h)\n",
      "        h_t = (1 - z_t) * h_tm1 + z_t * can_h_t\n",
      "\n",
      "        h_t = x_m[:, None] * h_t + (1. - x_m[:, None])*h_tm1\n",
      "        return h_t\n",
      "\n",
      "\n",
      "    def _step_forward(self, x_t, x_m, h_tm1):\n",
      "        '''\n",
      "        x_t: input at time t\n",
      "        x_m: mask of x_t\n",
      "        h_tm1: previous state\n",
      "        c_x: contex of the rnn\n",
      "        '''\n",
      "        z_t = T.nnet.sigmoid(T.dot(x_t, self.W_xz) +\n",
      "                             T.dot(h_tm1, self.W_hz) + self.b_z)\n",
      "\n",
      "        r_t = T.nnet.sigmoid(T.dot(x_t, self.W_xr) +\n",
      "                             T.dot(h_tm1, self.W_hr) + self.b_r)\n",
      "\n",
      "        can_h_t = T.tanh(T.dot(x_t, self.W_xh) +\n",
      "                         r_t * T.dot(h_tm1, self.W_hh) +\n",
      "                         self.b_h)\n",
      "        h_t = (1 - z_t) * h_tm1 + z_t * can_h_t\n",
      "\n",
      "        h_t = x_m[:, None] * h_t + (1. - x_m[:, None])*h_tm1\n",
      "        return h_t\n",
      "\n",
      "    def apply(self, state_below, mask_below, init_state=None, context=None):\n",
      "        if state_below.ndim == 3:\n",
      "            batch_size = state_below.shape[1]\n",
      "            n_steps = state_below.shape[0]\n",
      "        else:\n",
      "            raise NotImplementedError\n",
      "\n",
      "\n",
      "        if self.with_contex:\n",
      "            if init_state is None:\n",
      "                init_state = T.tanh(theano.dot(context, self.W_c_init))\n",
      "            c_z = theano.dot(context, self.W_cz)\n",
      "            c_r = theano.dot(context, self.W_cr)\n",
      "            c_h = theano.dot(context, self.W_ch)\n",
      "            non_sequences = [c_z, c_r, c_h]\n",
      "            rval, updates = theano.scan(self._step_forward_with_context,\n",
      "                                        sequences=[state_below, mask_below],\n",
      "                                        outputs_info=[init_state],\n",
      "                                        non_sequences=non_sequences,\n",
      "                                        n_steps=n_steps\n",
      "                                        )\n",
      "\n",
      "        else:\n",
      "            if init_state is None:\n",
      "                init_state = T.alloc(numpy.float32(0.), batch_size, self.n_hids)\n",
      "            rval, updates = theano.scan(self._step_forward,\n",
      "                                        sequences=[state_below, mask_below],\n",
      "                                        outputs_info=[init_state],\n",
      "                                        n_steps=n_steps\n",
      "                                        )\n",
      "        self.output = rval\n",
      "        return self.output\n",
      "\n",
      "    def merge_out(self, state_below, mask_below, context=None):\n",
      "        hiddens = self.apply(state_below, mask_below, context=context)\n",
      "        if context is None:\n",
      "            msize = self.n_in + self.n_hids\n",
      "            osize = self.n_hids\n",
      "            combine = T.concatenate([state_below, hiddens], axis=2)\n",
      "        else:\n",
      "            msize = self.n_in + self.n_hids + self.c_hids\n",
      "            osize = self.n_hids\n",
      "            n_times = state_below.shape[0]\n",
      "            m_context = repeat_x(context, n_times)\n",
      "            combine = T.concatenate([state_below, hiddens, m_context], axis=2)\n",
      "\n",
      "        self.W_m = param_init().uniform((msize, osize*2))\n",
      "        self.b_m = param_init().constant((osize*2,))\n",
      "        self.params += [self.W_m, self.b_m]\n",
      "\n",
      "        merge_out = theano.dot(combine, self.W_m) + self.b_m\n",
      "        merge_max = merge_out.reshape((merge_out.shape[0],\n",
      "                                       merge_out.shape[1],\n",
      "                                       merge_out.shape[2]/2,\n",
      "                                       2), ndim=4).max(axis=3)\n",
      "        return merge_max * mask_below[:, :, None]\n",
      "\n",
      "\n",
      "class lookup_table(object):\n",
      "    def __init__(self, embsize, vocab_size):\n",
      "        self.W = param_init().uniform((vocab_size, embsize))\n",
      "        self.params = [self.W]\n",
      "        self.vocab_size = vocab_size\n",
      "        self.embsize = embsize\n",
      "\n",
      "    def apply(self, indices):\n",
      "        outshape = [indices.shape[i] for i\n",
      "                    in range(indices.ndim)] + [self.embsize]\n",
      "\n",
      "        return self.W[indices.flatten()].reshape(outshape)\n",
      "\n",
      "\n",
      "class auto_encoder(object):\n",
      "    def __init__(self, sentence, sentence_mask, vocab_size, n_in, n_hids, **kwargs):\n",
      "        layers = []\n",
      "\n",
      "        #batch_size = sentence.shape[1]\n",
      "        encoder = GRU(n_in, n_hids, with_contex=False)\n",
      "        layers.append(encoder)\n",
      "\n",
      "        if 'table' in kwargs:\n",
      "            table = kwargs['table']\n",
      "        else:\n",
      "            table = lookup_table(n_in, vocab_size)\n",
      "        # layers.append(table)\n",
      "\n",
      "        state_below = table.apply(sentence)\n",
      "        context = encoder.apply(state_below, sentence_mask)[-1]\n",
      "\n",
      "        decoder = GRU(n_in, n_hids, with_contex=True)\n",
      "        layers.append(decoder)\n",
      "\n",
      "        decoder_state_below = table.apply(sentence[:-1])\n",
      "        hiddens = decoder.merge_out(decoder_state_below,\n",
      "                                    sentence_mask[:-1], context=context)\n",
      "\n",
      "        logistic_layer = LogisticRegression(hiddens, n_hids, vocab_size)\n",
      "        layers.append(logistic_layer)\n",
      "\n",
      "        self.cost = logistic_layer.cost(sentence[1:],\n",
      "                                        sentence_mask[1:])/sentence_mask[1:].sum()\n",
      "        self.output = context\n",
      "        self.params = []\n",
      "        for layer in layers:\n",
      "            self.params.extend(layer.params)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Memory(object):\n",
      "    \"\"\"\n",
      "    author:liuxianggen\n",
      "    \"\"\"\n",
      "    def __init__(self, **kwargs):\n",
      "        self.vocab_size = kwargs.pop('vocab_size')\n",
      "        self.n_in = kwargs.pop('nemb')\n",
      "        self.n_hids = kwargs.pop('nhids')\n",
      "        self.n_layer = kwargs.pop('n_layer')\n",
      "        self.n_label = kwargs.pop('label_size')\n",
      "        self.params=[]\n",
      "\n",
      "    def apply(self, facts, facts_mask, question, question_mask, y):\n",
      "\n",
      "        table = lookup_table(self.n_in, self.vocab_size)\n",
      "        self.params += table.params\n",
      "\n",
      "        facts_encoder = auto_encoder(facts, facts_mask, self.vocab_size,\n",
      "                                     self.n_in, self.n_hids, table=table)\n",
      "        self.eval = facts_encoder.output\n",
      "        return self.eval\n",
      "    \n",
      "    \n",
      "\n",
      "# \tdef append(self, entry):\n",
      "# \t\tself.entries.append(entry)\n",
      "\n",
      "# \tdef read(self, index):\n",
      "# \t\treturn self.entries[index]\n",
      " #####################\n",
      "x = T.lmatrix()\n",
      "x_mask = T.matrix()\n",
      "y = T.lmatrix()\n",
      "y_mask = T.matrix()\n",
      "l = T.lvector()\n",
      "\n",
      "config = getattr(config, 'get_config')()\n",
      "memory = Memory(**config)\n",
      "context = memory.apply(x, x_mask, y, y_mask, l)\n",
      "\n",
      "print memory.n_in\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "When compiling the inner function of scan the following error has been encountered: The initial state (`outputs_info` in scan nomenclature) of variable IncSubtensor{Set;:int64:}.0 (argument number 2) has dtype float32, while the result of the inner function (`fn`) has dtype float64. This can happen if the inner function of scan results in an upcast or downcast.",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-5-cc8fd4cb5205>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-3-0703ff337964>\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, facts, facts_mask, question, question_mask, y)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         facts_encoder = auto_encoder(facts, facts_mask, self.vocab_size,\n\u001b[0;32m---> 19\u001b[0;31m                                      self.n_in, self.n_hids, table=table)\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfacts_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-2-38a957d0aaa5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentence, sentence_mask, vocab_size, n_in, n_hids, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mstate_below\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_below\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_contex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-2-38a957d0aaa5>\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, state_below, mask_below, init_state, context)\u001b[0m\n\u001b[1;32m    162\u001b[0m                                         \u001b[0msequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate_below\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_below\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                                         \u001b[0moutputs_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minit_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                                         \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m                                         )\n\u001b[1;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Theano-0.9.0dev2-py2.7.egg/theano/scan_module/scan.pyc\u001b[0m in \u001b[0;36mscan\u001b[0;34m(fn, sequences, outputs_info, non_sequences, n_steps, truncate_gradient, go_backwards, mode, name, profile, allow_gc, strict)\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         \u001b[0mscan_inputs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m     \u001b[0mscan_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscan_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscan_outs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0mscan_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mscan_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Theano-0.9.0dev2-py2.7.egg/theano/gof/op.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \"\"\"\n\u001b[1;32m    601\u001b[0m         \u001b[0mreturn_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'return_list'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_test_value\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'off'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Theano-0.9.0dev2-py2.7.egg/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mmake_node\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m    536\u001b[0m                                   \u001b[0margoffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m                                   \u001b[0mouter_sitsot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m                                   inner_sitsot_out.type.dtype))\n\u001b[0m\u001b[1;32m    539\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minner_sitsot_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mouter_sitsot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m                 raise ValueError(err_msg3 %\n",
        "\u001b[0;31mValueError\u001b[0m: When compiling the inner function of scan the following error has been encountered: The initial state (`outputs_info` in scan nomenclature) of variable IncSubtensor{Set;:int64:}.0 (argument number 2) has dtype float32, while the result of the inner function (`fn`) has dtype float64. This can happen if the inner function of scan results in an upcast or downcast."
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "config = getattr(config, 'get_config')()test_files = config['test_file']\n",
      "print train_files # ['../data/19.train.fact', '../data/19.train.ques', '../data/19.train.answer']\n",
      "print test_files\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['/home/agen/workspace-python/RL_reasoning/data/19.train.fact', '/home/agen/workspace-python/RL_reasoning/data/19.train.ques', '/home/agen/workspace-python/RL_reasoning/data/19.train.answer']\n",
        "['/home/agen/workspace-python/RL_reasoning/data/19.test.fact', '/home/agen/workspace-python/RL_reasoning/data/19.test.ques', '/home/agen/workspace-python/RL_reasoning/data/19.test.answer']\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_class = preprocess(*train_files)\n",
      "test_class = preprocess(*test_files)\n",
      "for facts, question, label in data_class.data_stream():\n",
      "    print facts[0]\n",
      "    break\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 9 21  0  2 14 20 21 16 11  5]\n",
        " [ 9 21 19  2 17 20 21  0 11  5]\n",
        " [ 9 21 18  2 10 20 21  3 11  5]\n",
        " [ 9 21  0  2 10 20 21 18 11  5]\n",
        " [ 9 21  6  2 17 20 21 18 11  5]]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}